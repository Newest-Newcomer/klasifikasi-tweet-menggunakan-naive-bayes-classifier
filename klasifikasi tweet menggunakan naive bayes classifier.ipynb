{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        label                                              tweet\n",
      "0       anger  Soal jln Jatibaru,polisi tdk bs GERTAK gubernu...\n",
      "1       anger  Sesama cewe lho (kayaknya), harusnya bisa lebi...\n",
      "2       happy  Kepingin gudeg mbarek Bu hj. Amad Foto dari go...\n",
      "3       anger  Jln Jatibaru,bagian dari wilayah Tn Abang.Peng...\n",
      "4       happy  Sharing pengalaman aja, kemarin jam 18.00 bata...\n",
      "5       anger  Dari sekian banyak thread yang aku baca, threa...\n",
      "6       happy  Sharing sama temen tuh emg guna bgt. Disaat lu...\n",
      "7     sadness  Orang lain kalau pake ponco itu buat jas hujan...\n",
      "8       anger  Contoh mereka yg gemar menyudutkan, teriak pal...\n",
      "9     sadness  Pulang udah H-4 lebaran dilema sekali. Seperti...\n",
      "10      anger  Betul Min rakyat Indonesia sekarang harapan ny...\n",
      "11      anger  Kalaupun fansite ngejual hasil jepretan mereka...\n",
      "12      happy  sangat bersyukur bisa mendoakan kakeknya, Bung...\n",
      "13    sadness  Sulit menerima kenyataan memang bahwa ada seba...\n",
      "14      anger  samanye udeh maling semua gak si jancok gak si...\n",
      "15      anger  Ku juga pengen ngamuk bacain komen2 netizen yg...\n",
      "16       love  Setiap kesempatan yg pernah hadir tuk dapat me...\n",
      "17      anger  Sedap kali kau ya bobok dikasur ku. Aku tidur ...\n",
      "18      happy  H-9. Mau ke bank rame bgt ampe antrian tempat ...\n",
      "19      anger  Benar2 hancur negara ini dikelola amatiran, tp...\n",
      "20      anger  Budayakan mencari dan membaca sebelum nanya ya...\n",
      "21    sadness  Separuh hati ini iri jika melihat seorang anak...\n",
      "22      happy  Ketika aku tersenyum bukan berarti hidupku sem...\n",
      "23    sadness  dari mau tdr, tidur, sampe bgn tdr kok yaa kay...\n",
      "24       love  kan kupeluk engkau erat2 hingga tak ada seoran...\n",
      "25      anger  Kalo mau ganti presiden itu harus jelas siapa ...\n",
      "26      happy  Sukses n keren banget dgn no.1 kualitas,bhn kr...\n",
      "27      anger  Udah mau sarjana 2 kali, mbokya mulut nya jang...\n",
      "28      anger  Gimana orang ga nilai dr jilbab/syari/nggak ny...\n",
      "29       fear  Hari ini jadwal presentasi proker di LPPM karn...\n",
      "...       ...                                                ...\n",
      "2970     love  Dr dulu suka gemes sndr tiap abang maen game m...\n",
      "2971    happy  Terima Kasih, Terima kash banyak . Kepada Tuha...\n",
      "2972    anger  [USERNAME] Malam saya mau tanya kenapa benefit...\n",
      "2973     love  Cinta Ibu merupakan cinta yang paling suci Cin...\n",
      "2974  sadness  Gabisa sayang :( Kecuali anak s1 mau ke s2 psi...\n",
      "2975    happy  ataauuu aku yg ga pernah keluar rumah menjelan...\n",
      "2976     love  [USERNAME] Naomi..Aku benar-benar Sayang dan C...\n",
      "2977    anger  Sekali , dua kali , tiga kali , lama2 bosan ju...\n",
      "2978     love  Di sini Talha RA mati-matian berusaha ngelindu...\n",
      "2979    happy  happy birthday sender!! Wish you all the best,...\n",
      "2980     fear  jd kmrn dpt tmpt ujian di smp 5 trus pas surve...\n",
      "2981  sadness  dulu kalau mau berangkat ngaji trs hujan gak a...\n",
      "2982  sadness  Mencintai kamu itu tidak sulit, yang sulit itu...\n",
      "2983  sadness  [USERNAME] Udah dipaksa tapi aku bukan anak ge...\n",
      "2984    happy  AAAAAK SUKA BANGET. feel sehabis nonton sama k...\n",
      "2985     love  sekarang kalo lihat yg lagi kasmaran ngomong s...\n",
      "2986     love  Terimakasih buat semuanya, saya sayang kamu su...\n",
      "2987     love  Mungkin ini aneh dan gak relevan tapi aku baru...\n",
      "2988     fear  Kalo ada luka terbuka jg lbh gede tuh peluang ...\n",
      "2989     love  Aku dulu pernah ka suka, sampe didatengin kesi...\n",
      "2990    anger  selamat jam pulang ngantor! selamat bermacet2 ...\n",
      "2991    anger  [USERNAME] Paket Saya dengan No resi JB0002742...\n",
      "2992    anger  Sama sekali nggak. Arab lokal nggak peduli kok...\n",
      "2993    anger  [USERNAME] [USERNAME] Keledai itu dibilang bod...\n",
      "2994  sadness  petani dieng rugi akibat bun upas - mungkin su...\n",
      "2995     fear  Pas Sma kalo pelajaran berikutnya bindo, ngibr...\n",
      "2996    anger  Tapi bahaya juga sih Cal kalau gitu Amit2 karn...\n",
      "2997     love  Cewek bawel karna dia perduli. cewek cemburu k...\n",
      "2998    happy  Jalan-jalan cari suasana baru dan pengetahuan ...\n",
      "2999     fear  [USERNAME] [USERNAME] Cerita satinah satu ini ...\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import dataset\n",
    "data = pd.read_csv('C:/Users/User/Documents/Pembelajaran Bahasa Alami/Proyek/klasifikasi-jurnal-menggunakan-naive-bayes-classifier/Twitter_Emotion_Dataset(Train).csv')\n",
    "print (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5</td>\n",
       "      <td>2994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>anger</td>\n",
       "      <td>Aku gak salah bila mohon pd [USERNAME], segera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>869</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                              tweet\n",
       "count    3000                                               3000\n",
       "unique      5                                               2994\n",
       "top     anger  Aku gak salah bila mohon pd [USERNAME], segera...\n",
       "freq      869                                                  2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 2 columns):\n",
      "label    3000 non-null object\n",
      "tweet    3000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 47.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_vars = []\n",
    "categorical_vars = ['label', 'tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweet = data[numerical_vars + categorical_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      3000\n",
       "unique        5\n",
       "top       anger\n",
       "freq        869\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tweet['label'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_tweet['label'].hist(edgecolor='black', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5</td>\n",
       "      <td>2994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>anger</td>\n",
       "      <td>Aku gak salah bila mohon pd [USERNAME], segera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>869</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                              tweet\n",
       "count    3000                                               3000\n",
       "unique      5                                               2994\n",
       "top     anger  Aku gak salah bila mohon pd [USERNAME], segera...\n",
       "freq      869                                                  2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tweet[categorical_vars].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-aac96e2a9180>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m  \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "\n",
    "import re\n",
    "import nltk.corpus\n",
    "from Unidecode                        import unidecode\n",
    "from nltk.tokenize                    import word_tokenize\n",
    "from nltk                             import SnowballStemmer\n",
    "from sklearn.feature_extraction.text  import TfidfVectorizer\n",
    "from sklearn.preprocessing            import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"tweet\"] = data[\"tweet\"].astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data['tweet'].tolist()\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeWords(listOfTokens, listOfWords):\n",
    "    return [token for token in listOfTokens if token not in listOfWords]\n",
    "\n",
    "def applyStemming(listOfTokens, stemmer):\n",
    "    return [stemmer.stem(token) for token in listOfTokens]\n",
    "\n",
    "def twoLetters(listOfTokens):\n",
    "    twoLetterWord = []\n",
    "    for token in listOfTokens:\n",
    "        if len(token) <= 2 or len(token) >= 21:\n",
    "            twoLetterWord.append(token)\n",
    "    return twoLetterWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processCorpus(corpus, language):   \n",
    "    stopwords = nltk.corpus.stopwords.words(language)\n",
    "    param_stemmer = SnowballStemmer(language)\n",
    "    other_words = [line.rstrip('\\n') for line in open('stopwords_scrapmaker.txt')] # Load .txt file line by line\n",
    "    \n",
    "    for document in corpus:\n",
    "        index = corpus.index(document)\n",
    "        corpus[index] = corpus[index].replace(u'\\ufffd', '8')   # Replaces the ASCII '�' symbol with '8'\n",
    "        corpus[index] = corpus[index].replace(',', '')          # Removes commas\n",
    "        corpus[index] = corpus[index].rstrip('\\n')              # Removes line breaks\n",
    "        corpus[index] = corpus[index].casefold()                # Makes all letters lowercase\n",
    "        \n",
    "        corpus[index] = re.sub('\\W_',' ', corpus[index])        # removes specials characters and leaves only words\n",
    "        corpus[index] = re.sub(\"\\S*\\d\\S*\",\" \", corpus[index])   # removes numbers and words concatenated with numbers IE h4ck3r. Removes road names such as BR-381.\n",
    "        corpus[index] = re.sub(\"\\S*@\\S*\\s?\",\" \", corpus[index]) # removes emails and mentions (words with @)\n",
    "        corpus[index] = re.sub(r'http\\S+', '', corpus[index])   # removes URLs with http\n",
    "        corpus[index] = re.sub(r'www\\S+', '', corpus[index])    # removes URLs with www\n",
    "        \n",
    "        corpus[index] = re.sub(r'[^\\w]|_',' ',corpus[index])    #remove punctuations\n",
    "        \n",
    "        corpus[index] = re.sub('[\\s]+', ' ', corpus[index])     #Remove additional white spaces\n",
    "\n",
    "        listOfTokens = word_tokenize(corpus[index])\n",
    "        twoLetterWord = twoLetters(listOfTokens)\n",
    "\n",
    "        listOfTokens = removeWords(listOfTokens, stopwords)\n",
    "        listOfTokens = removeWords(listOfTokens, twoLetterWord)\n",
    "        listOfTokens = removeWords(listOfTokens, other_words)\n",
    "        \n",
    "        listOfTokens = removeWords(listOfTokens, other_words)\n",
    "\n",
    "        corpus[index]   = \" \".join(listOfTokens)\n",
    "        corpus[index] = unidecode(corpus[index])\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "language = 'english'\n",
    "corpus = processCorpus(corpus, language)\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
